{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "from experiments.metrics import BenchmarkRunner\n",
    "import common.utils as utils\n",
    "\n",
    "def plot_metric_results(metric_name, F1, F2, F3, hm, num_dm_dec, dataset_fold=None, sub_fold=None, drop_index=None, save_figs=False, fig_name_prefix=\"figure\"):\n",
    "    \"\"\"\n",
    "    Generic plotting function for ASRS, ASPS, or AIOS results using the new project structure.\n",
    "\n",
    "    Args:\n",
    "        metric_name (str): One of {'asrs', 'asps', 'aios'}.\n",
    "        F1, F2, F3: Parameter lists (taking the first element of each).\n",
    "        hm (int): Number of human models (used for validation/logging, though loader detects actual count).\n",
    "        num_dm_dec (int): Number of DM preferences (max steps).\n",
    "        dataset_fold (str): Folder name (e.g., 'datasets').\n",
    "        sub_fold (str): Subfolder name (e.g., 'BAYES_BT_BALD').\n",
    "        drop_index (int, optional): Index of specific Human Model to drop (outlier removal).\n",
    "        save_figs (bool, optional): If True, saves figures as PNG files.\n",
    "        fig_name_prefix (str, optional): Prefix for saved figure filenames.\n",
    "    \"\"\"\n",
    "    metric_name = metric_name.lower()\n",
    "    valid_metrics = ['asrs', 'asps', 'aios', 'perc_inc']\n",
    "    if metric_name not in valid_metrics:\n",
    "        raise ValueError(f\"metric_name must be one of {valid_metrics}\")\n",
    "\n",
    "    # Get first configuration values\n",
    "    f1, f2, f3 = F1[0], F2[0], F3[0]\n",
    "    \n",
    "    # X-axis: 1 to num_dm_dec\n",
    "    x = np.arange(1, num_dm_dec + 1)\n",
    "\n",
    "    # --- Load Data using new utils.py ---\n",
    "    # Signature: load_test_results(test_name, dataset_fold, sub_fold, num_dm_dec, f1, f2, f3)\n",
    "    y, y_active = utils.load_test_results(metric_name, dataset_fold, sub_fold, num_dm_dec, f1, f2, f3)\n",
    "    \n",
    "    if y.size == 0 or y_active.size == 0:\n",
    "        print(f\"No data found for {metric_name} in {dataset_fold}/{sub_fold}\")\n",
    "        return\n",
    "\n",
    "    print(f\"Loaded {metric_name.upper()} data shapes:\", np.shape(y), np.shape(y_active))\n",
    "\n",
    "    # Drop index if required (removing specific human model outliers)\n",
    "    if drop_index is not None:\n",
    "        y = np.delete(y, drop_index, axis=1)\n",
    "        y_active = np.delete(y_active, drop_index, axis=1)\n",
    "        print(f\"After dropping index {drop_index}:\", np.shape(y))\n",
    "\n",
    "    # --- Compute Statistics (Mean & 95% Confidence Interval) ---\n",
    "    # Axis 0 = Steps (Constraints), Axis 1 = Human Models (Runs)\n",
    "    n_samples = y.shape[1] \n",
    "    \n",
    "    mean = np.mean(y, axis=1)\n",
    "    # Std Error * 2 (approx 95% CI). Denominator is sqrt(N_runs), not len(mean)\n",
    "    std = np.std(y, axis=1) * 2 / np.sqrt(n_samples)\n",
    "    \n",
    "    mean_active = np.mean(y_active, axis=1)\n",
    "    std_active = np.std(y_active, axis=1) * 2 / np.sqrt(n_samples)\n",
    "\n",
    "    # Ensure folder exists\n",
    "    save_dir = os.path.join(\"figs\", metric_name)\n",
    "    if save_figs:\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    # --- Plot 1: Mean ± Std ---\n",
    "    plt.figure(figsize=(9, 5))\n",
    "    \n",
    "    # Regular (Passive)\n",
    "    plt.plot(x, mean, label=\"Regular\", color=\"blue\")\n",
    "    plt.fill_between(x, mean - std, mean + std, color=\"blue\", alpha=0.3, label=\"95% CI Regular\")\n",
    "\n",
    "    # Active\n",
    "    plt.plot(x, mean_active, label=\"Active\", color=\"orange\")\n",
    "    plt.fill_between(x, mean_active - std_active, mean_active + std_active, color=\"orange\", alpha=0.3, label=\"95% CI Active\")\n",
    "\n",
    "    plt.xlabel(\"Number of DM Preferences\", fontsize=12)\n",
    "    plt.ylabel(metric_name.upper(), fontsize=12)\n",
    "    plt.title(f\"{metric_name.upper()} Evolution (N={n_samples})\", fontsize=14)\n",
    "    plt.grid(True, linestyle=\"--\", alpha=0.6)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_figs:\n",
    "        plt.savefig(os.path.join(save_dir, f\"{fig_name_prefix}_mean_std.png\"), dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "    # --- Plot 2: Ratio ---\n",
    "    # Ratio > 1 means Active is higher (Better for Stability metrics usually)\n",
    "    # Handle division by zero safely\n",
    "    ratio = np.divide(mean_active, mean, out=np.ones_like(mean_active), where=mean != 0)\n",
    "\n",
    "    plt.figure(figsize=(9, 5))\n",
    "    plt.plot(x, ratio, label=f\"Active / Regular\", color=\"purple\")\n",
    "    plt.axhline(y=1, color=\"black\", linestyle=\"--\", linewidth=1)\n",
    "    plt.xlabel(\"Number of DM Preferences\", fontsize=12)\n",
    "    plt.ylabel(\"Ratio\", fontsize=12)\n",
    "    plt.title(f\"{metric_name.upper()} Ratio (Active / Regular)\", fontsize=14)\n",
    "    plt.grid(True, linestyle=\"--\", alpha=0.6)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_figs:\n",
    "        plt.savefig(os.path.join(save_dir, f\"{fig_name_prefix}_ratio.png\"), dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "def alg_meth(s):\n",
    "    \"\"\"Parses subfolder string into Algorithm and Method.\"\"\"\n",
    "    parts = s.split('_', 2) \n",
    "    before_second = '_'.join(parts[:2])\n",
    "    after_second = parts[2] if len(parts) > 2 else ''\n",
    "    return before_second.replace('_','-'), after_second"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BAYES-LIN US\n"
     ]
    }
   ],
   "source": [
    "# N. of alternatives\n",
    "F1 = [30]\n",
    "# N. of criteria\n",
    "F2 = [4]\n",
    "# the ratio (in %) of the number of pairwise comparisons provided by the DM to the number of all pairwise comparisons among alternatives\n",
    "F3 = [25]\n",
    "\n",
    "sub_folds = ['BAYES_BT_BALD', #0\n",
    "            'VBBAYES_LIN_BALD', #1\n",
    "             'BAYES_BT_US', #2\n",
    "             'BAYES_LIN_BALD', #3\n",
    "             'BAYES_LIN_US', #4\n",
    "             'FTRL_BT_BALD', #5\n",
    "             'FTRL_BT_US', #6\n",
    "             'VBFTRL_LIN_BALD', #7\n",
    "             'FTRL_LIN_BALD', #8\n",
    "             'FTRL_LIN_US' #9\n",
    "             ]\n",
    "\n",
    "dataset_folds = ['datasets']\n",
    "dataset_fold = dataset_folds[0]\n",
    "\n",
    "sub_fold = sub_folds[4]\n",
    "alg, active_method = alg_meth(sub_fold)\n",
    "print(alg, active_method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Calculating Metrics ===\n",
      "Calculating perc_inc...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  6.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating pois...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 75.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating rais...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 199.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating asrs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  6.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating aios...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  9.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating asps...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 10.45it/s]\n"
     ]
    }
   ],
   "source": [
    "hm = 1\n",
    "num_dm_dec = 109\n",
    "\n",
    "CALCULATE_METRICS = True\n",
    "#CALCULATE_METRICS = False\n",
    "\n",
    "# 2. Calculate Metrics\n",
    "if CALCULATE_METRICS:\n",
    "    print(\"\\n=== Calculating Metrics ===\")\n",
    "    # Note: Using first F1/F2/F3 config for metric calculation setup\n",
    "    f1, f2, f3 = F1[0], F2[0], F3[0]\n",
    "    num_dm_dec = int(np.round(f3 * (f1 * (f1 - 1) / 200)))\n",
    "    \n",
    "    runner = BenchmarkRunner(\n",
    "        dataset_fold=dataset_fold,\n",
    "        sub_fold=sub_fold, # Metrics for first method in list\n",
    "        num_subint=3,\n",
    "        hm=hm, # Use same limit here\n",
    "        F1=F1, F2=F2, F3=F3,\n",
    "        num_dm_dec=num_dm_dec\n",
    "    )\n",
    "    \n",
    "    runner.compute_perc_inc(force=CALCULATE_METRICS)\n",
    "    runner.compute_metrics(\"poi\", force=CALCULATE_METRICS)\n",
    "    runner.compute_metrics(\"rai\", force=CALCULATE_METRICS)\n",
    "    runner.compute_asrs(force=CALCULATE_METRICS)\n",
    "    runner.compute_aios(force=CALCULATE_METRICS)\n",
    "    runner.compute_asps(force=CALCULATE_METRICS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No data found for perc_inc in datasets/BAYES_LIN_US\n",
      "No data found for asrs in datasets/BAYES_LIN_US\n",
      "No data found for asps in datasets/BAYES_LIN_US\n",
      "No data found for aios in datasets/BAYES_LIN_US\n"
     ]
    }
   ],
   "source": [
    "flag2 = True\n",
    "d_i = None\n",
    "plot_metric_results('perc_inc', F1, F2, F3, hm, num_dm_dec, dataset_fold=dataset_fold, sub_fold=sub_fold, drop_index=d_i, save_figs=flag2, fig_name_prefix=sub_fold)\n",
    "plot_metric_results('asrs', F1, F2, F3, hm, num_dm_dec, dataset_fold=dataset_fold, sub_fold=sub_fold, drop_index=d_i, save_figs=flag2, fig_name_prefix=sub_fold)\n",
    "plot_metric_results('asps', F1, F2, F3, hm, num_dm_dec, dataset_fold=dataset_fold, sub_fold=sub_fold, drop_index=d_i, save_figs=flag2, fig_name_prefix=sub_fold)\n",
    "plot_metric_results('aios', F1, F2, F3, hm, num_dm_dec, dataset_fold=dataset_fold, sub_fold=sub_fold, drop_index=d_i, save_figs=flag2, fig_name_prefix=sub_fold)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
